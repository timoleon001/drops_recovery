# Восстановление простого одностраничного дропа

Восстанавливать дроп можно напрямую сохраняя его с архива [web.archive.org](https://web.archive.org) через браузер или при помощи стороннего ПО например [wayback-machine-downloader](https://github.com/hartator/wayback-machine-downloader). Данная инструкция задает ориентир как действовать, но каждый дроп уникален и могут возникнут проблемы не описанные в данной инструкции, если не удается понять рекомендуется обсудить с коллегами, возможно решение проблемы уже есть.

## Последовательность действий при восстановлении простого одностраничника:
1. Ознакамливаемся с дропом который предстоит восстановить на сайте вебАрхива `https://web.archive.org/web/*/https://stomorey.com/`, где `*` означает показать архив за все время, а `https://stomorey.com/` сайт который нужно восстановить.

2. Детально ознакамливаемся со снимками сайта по дням перебирая даты снимков когда сайт еще работал, имел актуальную версию и соответствует тематике. Открывать ссылки только синего цвета, зеленым помечены редиректы.

![Скрин примера страница календарь восстановления дропа](images/img_01.png?raw=true)

3. Подобрав нужный снимок, перед сохранением, вносим изменения в адресную строку, где указана дата снимка `20171221145549` в конец добавить `id_`. Это поможет убрать из кода ссылки на `web.archive.org`. Полученный урл будет иметь вид `https://web.archive.org/web/20171213174642id_/http://stomorey.com:80/`. Открыть код страницы `(CTRL + U)` выделить все `(CTRL + А)` и скопировать в буфер `(CTRL + С)`

4. Локально создать папку проекта и файл `index.html` куда добавляем скопированные данные с вебАрхива, сохраняем и открываем в браузере.
Для удобства можно создавать локально одноименный домен для проверки полученного результата например используя [Open Server](https://ospanel.io/) для систем **Windows**

5. Произвести чистку `html` от мусора и стороннего кода такого как яндекс, гугл статистики и друго. Привести `html` к стандарту **HTML5**, если код не соответствует. [Пример html кода страницы](index.html)

6. Открыть этот домен или просто файл index.html в браузере и через консоль можно посмотреть все недостающие файлы.

![Скрин отображения ошибок битых файлов для дальнейшего восстановления](images/img_02.png?raw=true)

Открываем по очереди недостающие файлы в новой вкладке браузера и в начало подставляем урл вебАрхива с соответствующей датой.
Например для `http://stomorey.com/css/style.css` получим `https://web.archive.org/web/20171020024130id_/http://stomorey.com/css/style.css`

Полученные данные сохраняем в проект в соответствующие папки и файлы. Если на сайте вебАрхива нет изображения, то можно подобрать по тематики из поиска гугла и сделать его при помощи скриншота, для того чтобы изображение стало уникальным, напрямую сохранять с поисковика запрещено.

Если нет `js` или `css`, как правило это будут файлы библиотек, таких как `jQuery`, то можно перезаливать из соответствующей библиотеки или `темы WP`, если дроп был на WP и найти недостающий файл.

Если получаем большое количество битых статических файлов, для ускорения восстановления можно воспользоваться [wayback-machine-downloader](https://github.com/hartator/wayback-machine-downloader)


### Основные команды для загрузки через wayback-machine
```
Весь домен с базовыми настройками
wayback_machine_downloader https://site.com

Весь домен за определенную дату
wayback_machine_downloader https://site.com --from 20230205104451

Качаем всю статику когда сайт на WP
wayback_machine_downloader https://site.com/wp-content
или только картинки
wayback_machine_downloader https://site.com/wp-content/uploads
или можно добавить определенную дату
wayback_machine_downloader https://site.com/wp-content --from 20230205104451
```

7. Вносимые правки должны принести минимальное влияние на внешний вид страниц, т.е. страница должна быть максимально похожа на оригинал с вебАрхива, кроме мобильной версии, которая может отсутствовать в оригинале.

8. Убедиться, что в коде не осталось внешних ссылок. Все битые и внешние ссылки заменяем на `#`. У изображений проверяем заполненные атрибуты `alt`, `height` и `width`, желательно, особенно изображение превышающие **100 кб** преобразовать в формат `webp`. Ссылки на изображение делаем относительными, т.е. убрать домен.

9. Проверить полученный результат на адаптивность и в случае его отсутствия обязательно адаптировать контент под мобильные устройства.

10. Добавить **robots.txt**, .**htaccess** и **sitemap.xml**

Для удобства открытия сайта на индексирование и проверки рабочего *robots* можно создавать их два варианта **robots.txt** и **robots-indexed.txt**
После положительной проверки и разрешение от менеджера открытие сайта на индексирование просто удалить **robots.txt**, а **robots-indexed.txt** переименовываем в **robots.txt**

Примеры
[robots.txt](robots.txt) - сайт закрыт на индексирование
[robots-indexed.txt](robots-indexed.txt) - на сайте разрешено индексирование поисковыми роботами, пример для WP
[.htaccess](.htaccess)
[sitemap_index.xml](sitemap_index.xml)
[post-sitemap.xml](post-sitemap.xml)
[images_sitemap.xml](images_sitemap.xml)

`post-sitemap.xml` и `images_sitemap.xml` удобно создавать при помощи  **Screaming Frog**

11. Проверяем проект через **Screaming Frog** и/или **SiteAnalyzer**

    При проверки обратить внимание на:
    - отсутствие битых ссылок и редиректов (у всех страниц код ответа **200**)
    - дубли или отсутствие важных тегов - **title**, **description**, **h1**, **canonical**, **X-robots-tag**
    - есть один уникальный заголовок **h1** и желательно не пересекается с **title**
    - содержимое нормально отображено после отключения **JavaScript**
    - есть значок **Favicon**
    - правильные **MIME-types**

12. Переименовываем локально папку проекта в отличную от оригинального домена, чтобы в браузере начал отображаться сайт с хостинга, а не с локальной папки.

13. Архивируем проект, заливаем на хостинг и еще раз проверяем через Screaming Frog + [pagespeed](https://pagespeed.web.dev/) + [httpstatus](https://httpstatus.io/) + [validator](https://validator.w3.org) (не должно быть `Error` сообщения с метками `Warning` и `Info` допускаются)

14. После положительных результатов всех тестов сдаем на проверку



## Восстановление многостраничного дропа
При восстановлении многостраничного дропа с большим количеством файлов принцип подхода анализа и проверки такой же как и для одностраничника, а сам процесс восстановления и редактирования файлов необходимо максимально автоматизировать. В работе использовать подход массового поиска и замены.
Пример в **VSCode**
![Скрин примера массовая замена](images/img_03.png?raw=true)
Для сложных задач, есть набор [Python скриптов](https://github.com/timoleon001/Python_Scripts.git), где скрыпты сгрупированы по папкам, а в названии заложен краткий ответ, что делает скрипт.


1. Проводим анализ дропа на сайте [web.archive.org](https://web.archive.org)


2. Качаем архив при помощи [wayback-machine-downloader](https://github.com/hartator/wayback-machine-downloader) целиком или конкретной даты, в зависимости от того, что показал анализ. Убедиться, что результат загрузки совпал с ожиданиями после анализа дропа на вебАрхиве, в случае отрицательного результата повторить загрузку изменив настройки загрузки на нужные в зависимости от ситуации.


3. Создаем локально домен одноименный дропу и добавляем загруженный результат.


4. Проверяем все полученные страницы, удаляем ненужные файлы и папки [скриптом](https://github.com/timoleon001/Python_Scripts/blob/main/delete/delete_empty_directory.py), а затем еще визуально пересмотреть. Если в задачи был список важных страниц, то необходимо оставить только их, а все лишние не вошедшие в этот список удалить. В формировании списка всех полученых страниц поможет [скрипт](https://github.com/timoleon001/Python_Scripts/blob/main/find/find_all_html_in_directory/get_all_path_to_files_and_save_list.py)


5. Вносим все необходимый правки по очистки мусора и преобразования страниц, чтобы они подходили требованиям и выполнить перелинковку между страницами (добиться результата, чтобы **Screaming Frog** видел все страницы)


6. После всех проверок и правок выполняем проверку на ошибки [скриптом](https://github.com/timoleon001/Python_Scripts/blob/main/main-multithreading/main-multithreading.py)


7. Отключаем локальный домен, архивируем, заливаем на хостинг.


8. Выполняем проверки сервисами **Screaming Frog (SiteAnalyzer)**, [pagespeed](https://pagespeed.web.dev/), [httpstatus](https://httpstatus.io/), [validator](https://validator.w3.org) (не должно быть `Error` сообщения с метками `Warning` и `Info` допускаются)


9. После положительных результатов всех тестов сдаем на проверку
